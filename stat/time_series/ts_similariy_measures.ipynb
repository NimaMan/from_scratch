{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the similarity measures between time series. We will use the following similarity measures:\n",
    "- Euclidean distance \n",
    "   The Euclidean distance between two time series is the square root of the sum of the squared differences between the two time series. It is formulated as follows:\n",
    "    $$d_{euclidean}(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}$$\n",
    "\n",
    "    we use the following function to calculate the Euclidean distance between two time series:\n",
    "    ```python\n",
    "    def euclidean_distance(x, y):\n",
    "        return np.sqrt(np.sum((x - y)**2))\n",
    "    ```\n",
    "    The Euclidean distance is a good measure of similarity between two time series. However, it is sensitive to the magnitude of the time series. For example, if the magnitude of the time series is increased by a factor of 10, the Euclidean distance will also increase by a factor of 10. This is not desirable. To overcome this problem, we can normalize the time series before calculating the Euclidean distance. The normalized Euclidean distance is formulated as follows:\n",
    "    $$d_{euclidean}(x,y) = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - y_i)^2}{\\sum_{i=1}^{n}x_i^2}}$$\n",
    "\n",
    "    we use the following function to calculate the normalized Euclidean distance between two time series:\n",
    "    ```python\n",
    "    def normalized_euclidean_distance(x, y):\n",
    "        return np.sqrt(np.sum(((x - y)**2) / np.sum(x**2)))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "        return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "\n",
    "def normalized_euclidean_distance(x, y):\n",
    "        return np.sqrt(np.sum(((x - y)**2) / np.sum(x**2)))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Time Warping (DTW) \n",
    "\n",
    "Dynamic Time Warping (DTW) is a measure of similarity between two time series that may vary in time or speed. DTW is formulated as follows:\n",
    "     $$d_{dtw}(x,y) = \\min_{\\pi} \\sum_{i=1}^{n}d(x_{\\pi(i)},y_i)$$\n",
    "    \n",
    "where $\\pi$ is the optimal warping path between the two time series and dtw is the distance between the two time series along the optimal warping path $\\pi$. We calulate the dtw as follows: \n",
    "1. Calculate the distance between each point in the first time series and each point in the second time series. This is called the cost matrix.\n",
    "2. Calculate the cumulative cost matrix. The cumulative cost matrix is the cost matrix with the minimum cost path from the top left corner to each point in the cost matrix.\n",
    "3. Find the optimal warping path. The optimal warping path is the path that minimizes the sum of the distances between the two time series. The distance between two points in the time series is calculated using the Euclidean distance. We use the following function to calculate the DTW distance between two time series:\n",
    "\n",
    "```python\n",
    "     def dtw(\n",
    "           x: np.ndarray, \n",
    "           y: np.ndarray, \n",
    "           dist: Callable[[np.ndarray, np.ndarray], float] = euclidean_distance\n",
    "     ) -> Tuple[float, np.ndarray]:\n",
    "           \"\"\"Calculate the Dynamic Time Warping (DTW) of two time series.\n",
    "     \n",
    "           Args:\n",
    "                x (np.ndarray): The first time series.\n",
    "                y (np.ndarray): The second time series.\n",
    "                dist (Callable[[np.ndarray, np.ndarray], float], optional): The distance function. Defaults to euclidean_distance.\n",
    "     \n",
    "           Returns:\n",
    "                Tuple[float, np.ndarray]: The DTW distance and the optimal warping path.\n",
    "           \"\"\"\n",
    "           # Calculate the cost matrix.\n",
    "           cost_matrix = np.zeros((len(x), len(y)))\n",
    "           for i in range(len(x)):\n",
    "                for j in range(len(y)):\n",
    "                     cost_matrix[i, j] = dist(x[i], y[j])\n",
    "     \n",
    "           # Calculate the cumulative cost matrix.\n",
    "           cumulative_cost_matrix = np.zeros((len(x), len(y)))\n",
    "           cumulative_cost_matrix[0, 0] = cost_matrix[0, 0]\n",
    "           for i in range(1, len(x)):\n",
    "                cumulative_cost_matrix[i, 0] = cumulative_cost_matrix[i - 1, 0] + cost_matrix[i, 0]\n",
    "           for j in range(1, len(y)):\n",
    "                cumulative_cost_matrix[0, j] = cumulative_cost_matrix[0, j - 1] + cost_matrix[0, j]\n",
    "           for i in range(1, len(x)):\n",
    "                for j in range(1, len(y)):\n",
    "                     cumulative_cost_matrix[i, j] = cost_matrix[i, j] + min(\n",
    "                          cumulative_cost_matrix[i - 1, j],\n",
    "                          cumulative_cost_matrix[i, j - 1],\n",
    "                          cumulative_cost_matrix[i - 1, j - 1],\n",
    "                     )\n",
    "     \n",
    "          # Find the optimal warping path using the most efficient method.\n",
    "          i = len(x) - 1\n",
    "          j = len(y) - 1\n",
    "          optimal_warping_path = [(i, j)]\n",
    "          while i > 0 and j > 0:\n",
    "               if i == 0:\n",
    "                    j -= 1\n",
    "               elif j == 0:\n",
    "                    i -= 1\n",
    "               else:\n",
    "                    if cumulative_cost_matrix[i - 1, j] < cumulative_cost_matrix[i, j - 1]:\n",
    "                         i -= 1\n",
    "                    else:\n",
    "                         j -= 1\n",
    "               optimal_warping_path.append((i, j))\n",
    "\n",
    "          optimal_warping_path.reverse()\n",
    "          return cumulative_cost_matrix[-1, -1], np.array(optimal_warping_path)\n",
    "```\n",
    "         \n",
    "The optimal warping path is the path that minimizes the sum of the distances between the two time series. The distance between two points in the time series is calculated using the Euclidean distance. We use the following function to calculate the DTW distance between two time series:\n",
    "\n",
    "```python\n",
    "     def dtw_distance(x, y):\n",
    "          return dtw(x, y, dist=lambda x, y: euclidean_distance(x, y))[0]\n",
    "```\n",
    "\n",
    "The DTW distance is a good measure of similarity between two time series. However, it is sensitive to the magnitude of the time series. For example, if the magnitude of the time series is increased by a factor of 10, the DTW distance will also increase by a factor of 10. This is not desirable. To overcome this problem, we can normalize the time series before calculating the DTW distance. The normalized DTW distance is formulated as follows:\n",
    "     $$d_{dtw}(x,y) = \\frac{1}{\\sum_{i=1}^{n}x_i^2}\\min_{\\pi} \\sum_{i=1}^{n}d(x_{\\pi(i)},y_i)$$\n",
    "    \n",
    "we use the following function to calculate the normalized DTW distance between two time series:\n",
    "\n",
    "```python\n",
    "     def normalized_dtw_distance(x, y):\n",
    "          return dtw(x, y, dist=lambda x, y: normalized_euclidean_distance(x, y))[0]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest Common Subsequence (LCSS) \n",
    "Longest Common Subsequence (LCSS) is a measure of similarity between two time series that may vary in time or speed. LCSS is formulated as follows:\n",
    "    $$d_{lcss}(x,y) = \\frac{1}{\\max\\{n,m\\}}\\sum_{i=1}^{n}\\sum_{j=1}^{m}1_{\\{d(x_i,y_j) \\leq \\epsilon\\}}$$\n",
    "    \n",
    "where $\\epsilon$ is the threshold distance and $1_{\\{d(x_i,y_j) \\leq \\epsilon\\}}$ is an indicator function that is equal to 1 if $d(x_i,y_j) \\leq \\epsilon$ and 0 otherwise. We calulate the lcss as follows: \n",
    "1. Calculate the distance between each point in the first time series and each point in the second time series. This is called the cost matrix.\n",
    "2. Calculate the cumulative cost matrix. The cumulative cost matrix is the cost matrix with the minimum cost path from the top left corner to each point in the cost matrix.\n",
    "3. Find the optimal warping path. The optimal warping path is the path that minimizes the sum of the distances between the two time series. The distance between two points in the time series is calculated using the Euclidean distance. We use the following function to calculate the LCSS distance between two time series:\n",
    "\n",
    "    ```python\n",
    "    def lcss(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        dist: Callable[[np.ndarray, np.ndarray], float] = euclidean_distance,\n",
    "        epsilon: float = 0.1\n",
    "    ) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"Calculate the Longest Common Subsequence (LCSS) of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "            dist (Callable[[np.ndarray, np.ndarray], float], optional): The distance function. Defaults to euclidean_distance.\n",
    "            epsilon (float, optional): The threshold distance. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, np.ndarray]: The LCSS distance and the optimal warping path.\n",
    "        \"\"\"\n",
    "        # Calculate the cost matrix.\n",
    "        cost_matrix = np.zeros((len(x), len(y)))\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(y)):\n",
    "                cost_matrix[i, j] = dist(x[i], y[j])\n",
    "        \n",
    "        # Calculate the cumulative cost matrix.\n",
    "        cumulative_cost_matrix = np.zeros((len(x), len(y)))\n",
    "        cumulative_cost_matrix[0, 0] = cost_matrix[0, 0]\n",
    "        for i in range(1, len(x)):\n",
    "            cumulative_cost_matrix[i, 0] = cumulative_cost_matrix[i - 1, 0] + cost_matrix[i, 0]\n",
    "        for j in range(1, len(y)):\n",
    "            cumulative_cost_matrix[0, j] = cumulative_cost_matrix[0, j - 1] + cost_matrix[0, j]\n",
    "        for i in range(1, len(x)):\n",
    "            for j in range(1, len(y)):\n",
    "                cumulative_cost_matrix[i, j] = cost_matrix[i, j] + min(\n",
    "                    cumulative_cost_matrix[i - 1, j],\n",
    "                    cumulative_cost_matrix[i, j - 1],\n",
    "                    cumulative_cost_matrix[i - 1, j - 1],\n",
    "                )\n",
    "\n",
    "        # Find the optimal warping path using the most efficient method.\n",
    "        i = len(x) - 1\n",
    "        j = len(y) - 1\n",
    "        optimal_warping_path = [(i, j)]\n",
    "        while i > 0 and j > 0:\n",
    "            if i == 0:\n",
    "                j -= 1\n",
    "            elif j == 0:\n",
    "                i -= 1\n",
    "            else:\n",
    "                if cumulative_cost_matrix[i - 1, j] < cumulative_cost_matrix[i, j - 1]:\n",
    "                    i -= 1\n",
    "                else:\n",
    "                    j -= 1\n",
    "            optimal_warping_path.append((i, j))\n",
    "\n",
    "        optimal_warping_path.reverse()\n",
    "        return cumulative_cost_matrix[-1, -1], np.array(optimal_warping_path)\n",
    "    ```\n",
    "\n",
    "The optimal warping path is the path that minimizes the sum of the distances between the two time series. The distance between two points in the time series is calculated using the Euclidean distance. We use the following function to calculate the LCSS distance between two time series:\n",
    "\n",
    "```python\n",
    "    def lcss_distance(x, y, epsilon=0.1):\n",
    "        return lcss(x, y, dist=lambda x, y: euclidean_distance(x, y), epsilon=epsilon)[0]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frechet distance \n",
    "Frechet distance is a measure of similarity between two time series that may vary in time or speed. Frechet distance is formulated as follows:\n",
    "    $$d_{frechet}(x,y) = \\min_{\\pi} \\max_{i=1}^{n}d(x_{\\pi(i)},y_i)$$\n",
    "    \n",
    "where $\\pi$ is the warping path. We calulate the frechet distance as follows:\n",
    "1. Calculate the distance between each point in the first time series and each point in the second time series. This is called the cost matrix.\n",
    "2. Calculate the cumulative cost matrix. The cumulative cost matrix is the cost matrix with the minimum cost path from the top left corner to each point in the cost matrix.\n",
    "3. Find the optimal warping path. The optimal warping path is the path that minimizes the sum of the distances between the two time series. The distance between two points in the time series is calculated using the Euclidean distance. We use the following function to calculate the Frechet distance between two time series:\n",
    "\n",
    "    ```python\n",
    "    def frechet(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        dist: Callable[[np.ndarray, np.ndarray], float] = euclidean_distance\n",
    "    ) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"Calculate the Frechet distance of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "            dist (Callable[[np.ndarray, np.ndarray], float], optional): The distance function. Defaults to euclidean_distance.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, np.ndarray]: The Frechet distance and the optimal warping path.\n",
    "        \"\"\"\n",
    "        # Calculate the cost matrix.\n",
    "        cost_matrix = np.zeros((len(x), len(y)))\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(y)):\n",
    "                cost_matrix[i, j] = dist(x[i], y[j])\n",
    "        \n",
    "        # Calculate the cumulative cost matrix.\n",
    "        cumulative_cost_matrix = np.zeros((len(x), len(y)))\n",
    "        cumulative_cost_matrix[0, 0] = cost_matrix[0, 0]\n",
    "        for i in range(1, len(x)):\n",
    "            cumulative_cost_matrix[i, 0] = cumulative_cost_matrix[i - 1, 0] + cost_matrix[i, 0]\n",
    "        for j in range(1, len(y)):\n",
    "            cumulative_cost_matrix[0, j] = cumulative_cost_matrix[0, j - 1] + cost_matrix[0, j]\n",
    "        for i in range(1, len(x)):\n",
    "            for j in range(1, len(y)):\n",
    "                cumulative_cost_matrix[i, j] = cost_matrix[i, j] + max(\n",
    "                    cumulative_cost_matrix[i - 1, j],\n",
    "                    cumulative_cost_matrix[i, j - 1],\n",
    "                    cumulative_cost_matrix[i - 1, j - 1],\n",
    "                )\n",
    "\n",
    "        # Find the optimal warping path using the most efficient method.\n",
    "        i = len(x) - 1\n",
    "        j = len(y) - 1\n",
    "        optimal_warping_path = [(i, j)]\n",
    "        while i > 0 and j > 0:\n",
    "            if i == 0:\n",
    "                j -= 1\n",
    "            elif j == 0:\n",
    "                i -= 1\n",
    "            else:\n",
    "                if cumulative_cost_matrix[i - 1, j] > cumulative_cost_matrix[i, j - 1]:\n",
    "                    i -= 1\n",
    "                else:\n",
    "                    j -= 1\n",
    "            optimal_warping_path.append((i, j))\n",
    "\n",
    "        optimal_warping_path.reverse()\n",
    "        return cumulative_cost_matrix[-1, -1], np.array(optimal_warping_path)\n",
    "    ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hausdorff distance \n",
    "Hausdorff distance is a measure of similarity between two time series that may vary in time or speed. Hausdorff distance is formulated as follows:\n",
    "    $$d_{hausdorff}(x,y) = \\max\\{d_{frechet}(x,y),d_{frechet}(y,x)\\}$$\n",
    "    \n",
    "where $\\pi$ is the warping path. We calulate the Hausdorff distance as follows:\n",
    "1. Calculate the distance between each point in the first time series and each point in the second time series. This is called the cost matrix.\n",
    "2. Calculate the cumulative cost matrix. The cumulative cost matrix is the cost matrix with the minimum cost path from the top left corner to each point in the cost matrix.\n",
    "3. Find the optimal warping path. The optimal warping path is the path that minimizes the sum of the distances between the two time series. The distance between two points in the time series is calculated using the Euclidean distance. We use the following function to calculate the Hausdorff distance between two time series:\n",
    "\n",
    "    ```python\n",
    "    def hausdorff(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray, \n",
    "        dist: Callable[[np.ndarray, np.ndarray], float] = euclidean_distance\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Hausdorff distance of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "            dist (Callable[[np.ndarray, np.ndarray], float], optional): The distance function. Defaults to euclidean_distance.\n",
    "\n",
    "        Returns:\n",
    "            float: The Hausdorff distance.\n",
    "        \"\"\"\n",
    "        return max(frechet(x, y, dist=dist)[0], frechet(y, x, dist=dist)[0])\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation distance\n",
    "## Pearson correlation coefficient\n",
    "Pearson correlation coefficient is a measure of the linear correlation between two time series. It is formulated as follows:\n",
    "    $$r^{pearson}_{x,y} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}}\\sqrt{\\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2}}}$$\n",
    "    \n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of the time series $x$ and $y$ respectively. We calulate the Pearson correlation coefficient as follows:\n",
    "    \n",
    "```python\n",
    "    def pearson_correlation_coefficient(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Pearson correlation coefficient of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "\n",
    "        Returns:\n",
    "            float: The Pearson correlation coefficient.\n",
    "        \"\"\"\n",
    "        # Calculate the mean of the time series.\n",
    "        x_mean = np.mean(x)\n",
    "        y_mean = np.mean(y)\n",
    "\n",
    "        # Calculate the numerator of the Pearson correlation coefficient.\n",
    "        numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "\n",
    "        # Calculate the denominator of the Pearson correlation coefficient.\n",
    "        denominator = np.sqrt(np.sum((x - x_mean) ** 2)) * np.sqrt(np.sum((y - y_mean) ** 2))\n",
    "\n",
    "        # Calculate the Pearson correlation coefficient.\n",
    "        return numerator / denominator\n",
    "```\n",
    "\n",
    "## Spearman correlation coefficient\n",
    "Spearman correlation coefficient is a measure of the monotonic correlation between two time series. It is formulated as follows:\n",
    "    $$r^{spearman}_{x,y} = \\frac{\\sum_{i=1}^{n}(r_{x}(x_{i}) - \\bar{r_{x}})(r_{y}(y_{i}) - \\bar{r_{y}})}{\\sqrt{\\sum_{i=1}^{n}(r_{x}(x_{i}) - \\bar{r_{x}})^{2}}\\sqrt{\\sum_{i=1}^{n}(r_{y}(y_{i}) - \\bar{r_{y}})^{2}}}$$\n",
    "    \n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of the time series $x$ and $y$ respectively. We calulate the Spearman correlation coefficient as follows:\n",
    "\n",
    "```python\n",
    "    def spearman_correlation_coefficient(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Spearman correlation coefficient of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "\n",
    "        Returns:\n",
    "            float: The Spearman correlation coefficient.\n",
    "        \"\"\"\n",
    "        # Calculate the rank of the time series.\n",
    "        x_rank = rankdata(x)\n",
    "        y_rank = rankdata(y)\n",
    "\n",
    "        # Calculate the mean of the time series.\n",
    "        x_mean = np.mean(x_rank)\n",
    "        y_mean = np.mean(y_rank)\n",
    "\n",
    "        # Calculate the numerator of the Spearman correlation coefficient.\n",
    "        numerator = np.sum((x_rank - x_mean) * (y_rank - y_mean))\n",
    "\n",
    "        # Calculate the denominator of the Spearman correlation coefficient.\n",
    "        denominator = np.sqrt(np.sum((x_rank - x_mean) ** 2)) * np.sqrt(np.sum((y_rank - y_mean) ** 2))\n",
    "\n",
    "        # Calculate the Spearman correlation coefficient.\n",
    "        return numerator / denominator\n",
    "```\n",
    "\n",
    "## Kendall correlation coefficient\n",
    "Kendall correlation coefficient is formulated as follows:\n",
    "    $$r^{kendall}_{x,y} = \\frac{2\\sum_{i=1}^{n}\\sum_{j=i+1}^{n}sgn(x_{i} - x_{j})sgn(y_{i} - y_{j})}{n(n - 1)}$$\n",
    "    \n",
    "where $sgn(x)$ is the sign function. We calulate the Kendall correlation coefficient as follows:\n",
    "\n",
    "```python\n",
    "    def kendall_correlation_coefficient(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Kendall correlation coefficient of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "\n",
    "        Returns:\n",
    "            float: The Kendall correlation coefficient.\n",
    "        \"\"\"\n",
    "        # Calculate the sign of the difference between each pair of points in the time series.\n",
    "        x_sign = np.sign(np.subtract.outer(x, x))\n",
    "        y_sign = np.sign(np.subtract.outer(y, y))\n",
    "\n",
    "        # Calculate the numerator of the Kendall correlation coefficient.\n",
    "        numerator = 2 * np.sum(x_sign * y_sign)\n",
    "\n",
    "        # Calculate the denominator of the Kendall correlation coefficient.\n",
    "        denominator = len(x) * (len(x) - 1)\n",
    "\n",
    "        # Calculate the Kendall correlation coefficient.\n",
    "        return numerator / denominator\n",
    "```\n",
    "\n",
    "### Kendels tau distance\n",
    "Kendels tau distance is formulated as follows:\n",
    "    $$d^{kendall}_{x,y} = 1 - \\frac{2\\sum_{i=1}^{n}\\sum_{j=i+1}^{n}sgn(x_{i} - x_{j})sgn(y_{i} - y_{j})}{n(n - 1)}$$\n",
    "    \n",
    "where $sgn(x)$ is the sign function. We calulate the Kendels tau distance as follows:\n",
    "\n",
    "```python\n",
    "    def kendall_tau_distance(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Kendels tau distance of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "\n",
    "        Returns:\n",
    "            float: The Kendels tau distance.\n",
    "        \"\"\"\n",
    "        return 1 - kendall_correlation_coefficient(x, y)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler divergence\n",
    "Kullback-Leibler divergence is formulated as follows:\n",
    "    $$d^{kl}_{x,y} = \\sum_{i=1}^{n}x_{i}\\log\\frac{x_{i}}{y_{i}}$$\n",
    "    \n",
    "where $x_{i}$ and $y_{i}$ are the $i$-th elements of the time series $x$ and $y$ respectively. We calulate the Kullback-Leibler divergence as follows:\n",
    "\n",
    "```python\n",
    "    def kullback_leibler_divergence(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Kullback-Leibler divergence of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "\n",
    "        Returns:\n",
    "            float: The Kullback-Leibler divergence.\n",
    "        \"\"\"\n",
    "        # Calculate the Kullback-Leibler divergence.\n",
    "        return np.sum(x * np.log(x / y))\n",
    "```\n",
    "## Jensen-Shannon divergence\n",
    "Jensen-Shannon divergence is formulated as follows:\n",
    "    $$d^{js}_{x,y} = \\frac{1}{2}d^{kl}_{x,\\frac{x + y}{2}} + \\frac{1}{2}d^{kl}_{y,\\frac{x + y}{2}}$$\n",
    "    \n",
    "where $d^{kl}_{x,y}$ is the Kullback-Leibler divergence. Jensen-Shannon divergence is a metric that is always greater than or equal to zero and is zero if and only if the two distributions are identical. It  differs from the Kullback-Leibler divergence in that it is symmetric. \n",
    "\n",
    "We calulate the Jensen-Shannon divergence as follows:\n",
    "\n",
    "```python\n",
    "    def jensen_shannon_divergence(\n",
    "        x: np.ndarray, \n",
    "        y: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate the Jensen-Shannon divergence of two time series.\n",
    "        Args:\n",
    "            x (np.ndarray): The first time series.\n",
    "            y (np.ndarray): The second time series.\n",
    "\n",
    "        Returns:\n",
    "            float: The Jensen-Shannon divergence.\n",
    "        \"\"\"\n",
    "        # Calculate the mean of the time series.\n",
    "        x_mean = np.mean(x)\n",
    "        y_mean = np.mean(y)\n",
    "\n",
    "        # Calculate the Jensen-Shannon divergence.\n",
    "        return 0.5 * kullback_leibler_divergence(x, (x_mean + y_mean) / 2) + 0.5 * kullback_leibler_divergence(y, (x_mean + y_mean) / 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bhattacharyya distance \n",
    "- Hellinger distance \n",
    "- Cosine similarity \n",
    "- Chebyshev distance \n",
    "- Minkowski distance \n",
    "- Bray-Curtis distance \n",
    "- Canberra distance \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jaccard distance - Weighted Euclidean distance - Weighted DTW - Weighted LCSS - Weighted Frechet distance - Weighted Hausdorff distance - Weighted Pearson correlation - Weighted Spearman correlation - Weighted Kendall correlation - Weighted Jensen-Shannon divergence - Weighted Kullback-Leibler divergence - Weighted Bhattacharyya distance - Weighted Hellinger distance - Weighted Cosine similarity - Weighted Manhattan distance - Weighted Chebyshev distance - Weighted Minkowski distance - Weighted Bray-Curtis distance - Weighted Canberra distance - Weighted Jaccard distance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05e50049e3eb32775174019135b7208a0d3852fb22829b3658213f387a3fdcbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
