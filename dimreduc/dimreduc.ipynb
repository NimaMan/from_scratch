{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimensionality reduction\n",
    "In this notebook, we will explore the dimensionality reduction techniques. We will use a sinmple dataset of 1000 samples with 100 features. We will use different dimensionality reduction techniques to reduce the dimensionality of the data and visualize the data in 2D space.\n",
    "Here is the list of dimensionality reduction techniques we will explore in this notebook:\n",
    "1. Principal Component Analysis (PCA)\n",
    "2. t-distributed Stochastic Neighbor Embedding (tSNE)\n",
    "3. Independent Component Analysis (ICA)\n",
    "4. Spectral Embedding\n",
    "5. Linear Discriminant Analysis (LDA)\n",
    "6. Isomap\n",
    "7. Locally Linear Embedding (LLE)\n",
    "8. Multidimensional Scaling (MDS)\n",
    "\n",
    "These methods have advantages and disadvantages. For example, PCA is very fast but it loses the finer details of the data after the reduction. On the other hand, tSNE is very slow but it preserves the underlying structure of the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA \n",
    "PCA preserves the Maximum Variance in the data. The pricipal components are the directions of the data with the maximum variance. The first principal component is the direction of the data with the maximum variance. Mathematically speaking, we are solving the following optimization problem:\n",
    "$$\n",
    "\\underset{w}{\\text{maximize}} \\frac{1}{n} \\sum_{i=1}^{n} (w^T x_i)^2\n",
    "$$\n",
    "where $w$ is the direction of the data with the maximum variance. The solution to this optimization problem is the first principal component. The second principal component is the direction of the data with the maximum variance that is orthogonal to the first principal component. Mathematically speaking, we are solving the following optimization problem:\n",
    "$$\n",
    "\\underset{w}{\\text{maximize}} \\frac{1}{n} \\sum_{i=1}^{n} (w^T x_i)^2\n",
    "$$\n",
    "subject to the constraint that $w$ is orthogonal to the first principal component. The constraint can be written as $w^T w_1 = 0$.\n",
    "\n",
    "## PCA in python from scratch\n",
    "We can implement PCA in python from scratch. Here is the code:\n",
    "```python\n",
    "def pca_vanilla(X, n_components):\n",
    "    # center the data\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    # calculate the covariance matrix\n",
    "    cov = np.cov(X.T)\n",
    "    # calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "    # sort the eigenvalues and eigenvectors in descending order\n",
    "    idx = eigenvalues.argsort()[::-1]   \n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:,idx]\n",
    "    # select the first n_components eigenvectors\n",
    "    eigenvectors = eigenvectors[:, :n_components]\n",
    "    # project the data onto the first n_components eigenvectors\n",
    "    X_transformed = np.dot(X, eigenvectors)\n",
    "    return X_transformed\n",
    "```\n",
    "### PCA using singular value decomposition (SVD)\n",
    "We can also implement PCA using singular value decomposition (SVD). Here is the code:\n",
    "```python\n",
    "def pca_svd(X, n_components):\n",
    "    # center the data\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    # calculate the covariance matrix\n",
    "    cov = np.cov(X.T)\n",
    "    # calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "    U, S, V = np.linalg.svd(cov)\n",
    "    # select the first n_components eigenvectors\n",
    "    U = U[:, :n_components]\n",
    "    # project the data onto the first n_components eigenvectors\n",
    "    X_transformed = np.dot(X, U)\n",
    "    return X_transformed\n",
    "```\n",
    "\n",
    "*** What does maximum variance in first principal component mean? ***\n",
    "It means that the data points are spread out as much as possible along the first principal component. \n",
    "Why is this important? Because the first principal component captures the maximum amount of information about the data. \n",
    "\n",
    "*** What does maximum variance in second principal component mean? ***\n",
    "It means that the data points are spread out as much as possible along the second principal component. \n",
    "\n",
    "*** Why do we need to preserve the maximum variance in the data? ***\n",
    "Because the data points are spread out as much as possible along the components. This means that the data are as informative as possible.\n",
    "\n",
    "*** How do we find which features are building the principal component? ***\n",
    "How to find features that are contributing the most to the variance of the data? How to find features that are contributing the most to the variance along the first principal component? This can be done by calculating the correlation between the features and the first principal component. The features with the highest correlation are the features that are contributing the most to the variance along the first principal component. In python we can use the following code to find the features that are contributing the most to the variance along the first principal component.\n",
    "```python\n",
    "def find_features_contributing_most_to_variance_along_first_principal_component(X):\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(X)\n",
    "    first_principal_component = pca.components_[0]\n",
    "    correlation = np.corrcoef(X.T, first_principal_component)\n",
    "    return np.argsort(correlation[0, :-1])[::-1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression data set with 1000 samples and 10 features and 5 informative features\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn import manifold\n",
    "from sklearn import decomposition\n",
    "from sklearn import random_projection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Z-score the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_svd(X, n_components, return_components=False):\n",
    "    # center the data\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    # calculate the covariance matrix\n",
    "    cov = np.cov(X.T)\n",
    "    # calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "    U, S, V = np.linalg.svd(cov)\n",
    "    # print the explained variance\n",
    "    explained_variance = np.cumsum(S**2) / np.sum(S**2)\n",
    "    print(\"Explained variance: \", explained_variance)\n",
    "    # select the first n_components eigenvectors\n",
    "    U = U[:, :n_components]\n",
    "    # project the data onto the first n_components eigenvectors\n",
    "    X_transformed = np.dot(X, U)\n",
    "    if return_components:\n",
    "        return X_transformed, U\n",
    "    return X_transformed\n",
    "\n",
    "def find_features_contributing_most_to_variance_to_components(X, components, n_features=5):\n",
    "    # calculate the variance of each feature\n",
    "    var = np.var(X, axis=0)\n",
    "    # calculate the variance of each component\n",
    "    var_comp = np.var(components, axis=0)\n",
    "    # calculate the contribution of each feature to each component\n",
    "    contribution = np.abs(np.dot(components.T, X.T) / var_comp[:, None])\n",
    "    # find the features that contribute most to each component\n",
    "    features = np.argsort(contribution, axis=0)[::-1, :n_features]\n",
    "    # print the features that contribute most to each component\n",
    "    for i in range(contribution.shape[1]):\n",
    "        print(\"Component {}: {}\".format(i, features[:, i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the features that are contributing the most to the variance along the first principal component?\n",
    "We can use the following code to find the features that are contributing the most to the variance along the first principal component.\n",
    "1. We can measure the correlation between the features and the principal components.\n",
    "```python\n",
    "def find_features_contributing_most_to_variance_along_first_principal_component(X, components_to_use):\n",
    "    correlation = np.corrcoef(X.T, components_to_use)\n",
    "    return np.argsort(correlation[0, :-1])[::-1]\n",
    "```\n",
    "2. Variance of the features along the component\n",
    "```python\n",
    "def find_features_contributing_most_to_variance_along_first_principal_component(X, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    first_principal_component = pca.components_[0]\n",
    "    variance = np.var(X * first_principal_component, axis=0)\n",
    "    return np.argsort(variance)[::-1]\n",
    "```\n",
    "\n",
    "#### Calculate the Loadings\n",
    "The loadings are the correlation between the features and the principal components. For each principal component, calculate the loading of each variable as the correlation between the variable and the principal component. Specifically, the loading for the i-th variable in the j-th principal component is given by:\n",
    "\n",
    "$$\n",
    "loading(i,j) = eigenvector(i,j) * sqrt(eigenvalue(j))\n",
    "$$\n",
    "\n",
    "where eigenvector(i,j) is the i-th element of the j-th eigenvector, and eigenvalue(j) is the j-th eigenvalue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance:  [0.90854251 0.9976569  0.99995423 1.        ]\n",
      "Component 0: [0 1 2 3]\n",
      "Component 1: [0 1 2 3]\n",
      "Component 2: [0 1 2 3]\n",
      "Component 3: [0 1 2 3]\n",
      "Component 4: [0 1 3 2]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m xt, U \u001b[39m=\u001b[39m pca_svd(X, n_components\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, return_components\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# find the features that contribute most to each component\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m find_features_contributing_most_to_variance_to_components(X, U, n_features\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[42], line 30\u001b[0m, in \u001b[0;36mfind_features_contributing_most_to_variance_to_components\u001b[0;34m(X, components, n_features)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m# print the features that contribute most to each component\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(contribution\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mComponent \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i, features[:, i]))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 5"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "xt, U = pca_svd(X, n_components=4, return_components=True)\n",
    "# find the features that contribute most to each component\n",
    "find_features_contributing_most_to_variance_to_components(X, U, n_features=5)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features contributing most to the first component: [0 2 3 1]\n",
      "Correlation between the original data and the projected data to the first component: -0.8901687648612954\n"
     ]
    }
   ],
   "source": [
    "X_transformed = np.dot(X, U)\n",
    "# Calculate the similarity between the original data and the projected data to the first component\n",
    "similarity = np.corrcoef(X.T, X_transformed[:, 0])\n",
    "# find the features that contribute most to the first component\n",
    "features = np.argsort(similarity[0, :-1])[::-1][:5]\n",
    "print(\"Features contributing most to the first component: {}\".format(features))\n",
    "\n",
    "# Print the correlation between the original data and the projected data to the first component\n",
    "print(\"Correlation between the original data and the projected data to the first component: {}\".format(similarity[0, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.11756978,  0.87175378,  0.81794113, -0.89016876],\n",
       "       [ 0.87175378, -0.4284401 ,  1.        ,  0.96286543, -0.99155518],\n",
       "       [ 0.81794113, -0.36612593,  0.96286543,  1.        , -0.96497896],\n",
       "       [-0.11756978,  1.        , -0.4284401 , -0.36612593,  0.46014271]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation = np.corrcoef(X.T, X_transformed[:, 0])\n",
    "correlation[np.argsort(correlation[0, :-1])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0: [-0.8901687648612947, 0.4601427064479079, -0.9915551834193612, -0.9649789606692488]\n",
      "Component 1: [-0.36082988811302424, -0.8827162691623837, -0.023415188379165508, -0.06399984704374657]\n",
      "Component 2: [0.27565766677723386, -0.09361987381838889, -0.054446991873719514, -0.24298265497845473]\n",
      "Component 3: [0.03760601888781117, -0.017776306845519417, -0.11534978224196103, 0.07535950121713178]\n"
     ]
    }
   ],
   "source": [
    "# calculate pearson correlation between two variables\n",
    "def pearson_correlation(x, y):\n",
    "    # calculate mean and standard deviation\n",
    "    x_mean, y_mean = np.mean(x), np.mean(y)\n",
    "    x_std, y_std = np.std(x), np.std(y)\n",
    "    # calculate covariance\n",
    "    covariance = np.sum((x - x_mean) * (y - y_mean)) / len(x)\n",
    "    # calculate correlation\n",
    "    correlation = covariance / (x_std * y_std)\n",
    "    return correlation\n",
    "\n",
    "# Print the correlation between the original data and the projected data to each component\n",
    "for i in range(U.shape[1]):\n",
    "    X_transformed = np.dot(X, U[:, i])\n",
    "    corr = [pearson_correlation(X[:, j], X_transformed) for j in range(X.shape[1])]\n",
    "    print (\"Component {}: {}\".format(i, corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
